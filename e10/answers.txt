1. How long did your reddit_averages.py take with (1) the reddit-0 data set and effectively no work, (2) no schema specified and not caching (on reddit-2 for this and the rest), (3) with a schema but not caching, (4) with both a schema and caching the twice-used DataFrame? [The reddit-0 test is effectively measuring the Spark startup time, so we can see how long it takes to do the actual work on reddit-2 in the best/worst cases.]

Reddit-0: 18.624s
No schema and no caching: 41.114s
Schema and no caching: 34.257s
Schema and caching: 27.270s

2. Based on the above, does it look like most of the time taken to process the reddit-2 data set is in reading the files, or calculating the averages?

If a cache and a schema aren't used then most of the time goes towards calculations, but with a schema and/or cache, most of the time goes towards reading the files.

3. Where did you use .cache() in your wikipedia_popular.py? [Hint: the answer had better be “once”… but where?]

I used it right before groupby because by that point the pages df will not be changed anymore and I do not want spark to recompute the pages df for the subsequent operations.